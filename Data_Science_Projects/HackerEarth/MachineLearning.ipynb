{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import PyPDF4\n",
    "import re\n",
    "import nltk\n",
    "import io\n",
    "import spacy\n",
    "from mosestokenizer import MosesTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_pdf = 'Data/training/Scripts_Philosophers Stone.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pdf_reader(path_to_pdf):\n",
    "    data = PyPDF4.PdfFileReader(open(path_to_pdf, 'rb'))\n",
    "    text = ''\n",
    "    for x in range(data.numPages):\n",
    "        obj = data.getPage(x)\n",
    "        mydata = obj.extractText()\n",
    "        for line in io.StringIO(mydata):\n",
    "            text +=line.replace('\\n',' ')\n",
    "    return text\n",
    "text = pdf_reader(path_to_pdf)\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_mystopwords(doc):\n",
    "#     doc = nlp(text)\n",
    "#     stop_words = nlp.Defaults.stop_words\n",
    "#     tokens_filtered= [str(word) for word in doc if not word.is_stop]\n",
    "#     return ' '.join(tokens_filtered)\n",
    "# docs = remove_mystopwords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with MosesTokenizer('en') as tokenizer:\n",
    "    sentence_list = tokenizer(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(analyzer='word' ,stop_words='english')\n",
    "# vectorizer.fit_transform(text_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "word_frequencies = {}\n",
    "for word in nltk.word_tokenize(text):\n",
    "    if word not in stopwords:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}\n",
    "for sent in sentence_list:\n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". , ? ... ! - Mr. know got like going think points right Come Good wizard one come want good boy day thing Know game course Right way gone great see bad find house better bit Got told broom said mean parents look tell something See little things knows wrong time Go go nice dear stop place found afternoon heard Let Look years na would sir knew get gave spell blood troll Yes magic Think people luck looking wo happened bloody sure kill gon dog One understand wand door dragon name Professor leave father need remember went 9 left follow year Bit trust important safe dead mind saying die job broomstick try seen minute head never let Tell Like must Stop family new mother met school tried night 10 Well please trying wanna guarding back means Troll wake birthday funny business Sorry letter today fine wonder died fool yes real cup begin ask read death relax eye moment bed catch live past steal kind ta stand sort famous away Okay Wait Follow mirror two Something Nice known life Hurry Help Fine wish killed vault secret wait happens Want change First join Sir taken locked ought first chess 50 detention\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import heapq\n",
    "summary_sentences = heapq.nlargest(200, sentence_scores, key=sentence_scores.get)\n",
    "summary = ' '.join(summary_sentences)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
